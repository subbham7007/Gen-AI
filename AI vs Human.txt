prompt = f"""
You are an advanced AI evaluator. You will receive three pieces of information for each customer query: the original user query, the solution provided by a Customer Application Engineer (CAE), and the solution generated by an AI model.

Your task is to evaluate the AI-generated solution based on the following criteria:

1. **Correctness:** Clearly state if the AI-generated solution is correct ("Yes" or "No"). A solution is correct if it aligns with the CAE solution in terms of technical accuracy.
2. **Acceptance:** Clearly state if the AI-generated solution is acceptable ("Yes" or "No"). A solution is acceptable if it provides helpful, relevant information that is likely to satisfy the user's query, even if it is not an exact match to the CAE solution.
3. **Similarity Score:** Rate how similar the AI-generated solution is to the CAE-provided solution on a scale from 1 to 10, where 1 is not similar at all and 10 is nearly identical.
4. **Justification:** Provide a detailed justification explaining your evaluation. Highlight specific areas where the AI-generated solution matches or deviates from the CAE-provided solution. Mention any strengths, such as relevance, usefulness, and completeness of the information, and any weaknesses, such as missing critical details or inaccuracies.
5. **Actionable Steps:** Indicate if the AI-generated solution provides clear, actionable steps that the user can follow.
6. **User Satisfaction Likelihood:** Predict how likely the user is to be satisfied with the AI-generated solution based on its relevance and completeness.
7. **Additional Guidance Needed:** Indicate whether the AI solution would benefit from further clarification or additional information.
8. **Comparison Highlights:** Provide a brief summary of the main points where the AI-generated solution aligns or diverges from the CAE solution.
9. **Overall Evaluation:** Offer an overall qualitative assessment of the AI-generated solution.

### Input:
User Query: \"\"\"{user_query}\"\"\"
CAE Solution: \"\"\"{cae_solution}\"\"\"
AI Generated Solution: \"\"\"{ai_generated_solution}\"\"\"

### Output (in JSON format):

```json
{{
  "Correctness": "<Yes/No>",
  "Acceptance": "<Yes/No>",
  "Similarity_Score": <Similarity_Score>,
  "Justification": "<Justification>",
  "Actionable_Steps": "<Yes/No>",
  "User_Satisfaction_Likelihood": "<High/Medium/Low>",
  "Additional_Guidance_Needed": "<Yes/No>",
  "Comparison_Highlights": "<Summary of similarities and differences>",
  "Overall_Evaluation": "<Excellent/Good/Fair/Poor>"
}}


"""



# For SelfCheckGPT

prompt = f"""
You are llama3.1-70b, a state-of-the-art language model developed by OpenAI. Your task is to evaluate the response you have provided to a given user query. Use the following criteria to assess your response and output the evaluation in JSON format only. For each evaluation metric, provide a score, rationale, and reason for the score.

1. **Accuracy**: Ensure that the information provided is factually correct and precise.
2. **Relevance**: Check that the response directly addresses the user's query without deviating from the topic.
3. **Clarity**: Verify that the response is clear, well-structured, and easy to understand.
4. **Depth**: Evaluate whether the response provides a comprehensive answer, covering all necessary aspects of the query.
5. **Tone**: Ensure that the response maintains a professional and appropriate tone for the context.
6. **Use of Examples**: Assess the inclusion and relevance of examples to illustrate points effectively.
7. **Coherence**: Ensure the response is logically structured and the flow of ideas is smooth and consistent.
8. **Originality**: Verify that the response includes unique insights or perspectives and is not merely a repetition of known facts.
9. **Fact-Checking**: Cross-reference the response with reliable sources to confirm the factual correctness of the information provided.
10. **Hallucination**: Ensure there is no fabrication of information that is not supported by the provided context or known facts.

### User Query:
{query}

### Model Response:
"{Model_Response}"

### Self-Evaluation:

Note: keep in mind to score between 1-10 whwere 0 is the lowest and 10 is the highest
1. **Accuracy**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The information provided is [accurate/inaccurate] because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

2. **Relevance**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The response is [relevant/irrelevant] to the user's query because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

3. **Clarity**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The response is [clear/unclear] because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

4. **Depth**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The response provides [comprehensive/insufficient] coverage of the query because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

5. **Tone**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The tone of the response is [appropriate/inappropriate] because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

6. **Use of Examples**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The examples used are [effective/ineffective] because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

7. **Coherence**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The response is [coherent/incoherent] because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

8. **Originality**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The response is [original/unoriginal] because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

9. **Fact-Checking**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The facts are [accurate/inaccurate] because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

10. **Hallucination**: 
   - **Score**: [Your Score Here]
   - **Rationale**: The response contains [no/many] fabricated information because [provide detailed explanation].
   - **Reason**: I gave this score because [explain how or why this score was given].

### Suggestions for Improvement:
- [Your Suggestions Here]

### Final Rating:
- Provide an overall rating for the response on a scale of 1 to 10, with 10 being perfect.
- **Score**: [Your Rating Here]
- **Reason**: I gave this final rating because [explain how or why this overall rating was given].

# Note: Dont include this also"Here is the self-evaluation in JSON format:" and ```json``` also
"""
